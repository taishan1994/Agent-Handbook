# 前言

为了让 AI 模型在特定情境下发挥作用，通常需要获得背景知识。例如，客户支持聊天机器人需要了解其所服务的具体业务，而法律分析机器人则需要了解大量过去案件。

开发者通常通过检索增强生成（RAG）来增强 AI 模型的知识。RAG 是一种从知识库中提取相关信息并将其附加到用户提示词中的方法，显著提升模型的响应能力。问题在于，传统的 RAG 解决方案在编码信息时会去除上下文，这常常导致系统无法从知识库中检索相关信息。

本文概述了一种显著改善 RAG 检索步骤的方法。该方法称为“上下文检索”，采用两个子技术：上下文嵌入和上下文 BM25。该方法可将失败检索次数减少 49%，结合重新排序则减少 67%。这些都显著提升了检索精度，直接转化为下游任务的更好表现。

# 关于简单使用更长提示的说明

有时候，最简单的解决方案就是最好的。如果你的知识库小于 20 万个 token（约 500 页材料），你可以直接把整个知识库写进给模型的提示词里，无需 RAG 或类似方法。然而，随着知识库的增长，你需要一个更具可扩展性的解决方案。这就是情境检索发挥作用的地方。

# RAG 入门：扩展到更大规模的知识库

对于不适合上下文窗口的大型知识库，RAG 是典型的解决方案。RAG 通过预处理知识库，步骤如下：
- 将知识库（文档的“语料库”）拆分成更小的文本块，通常不超过几百个标记;
- 使用嵌入模型将这些块转换为编码意义的向量嵌入;
- 将这些嵌入存储在一个矢量数据库中，允许通过语义相似性进行搜索。

运行时，当用户向模型输入查询时，向量数据库会根据与查询的语义相似性找到最相关的块。然后，最相关的片段被添加到发送给生成模型的提示中

虽然嵌入模型擅长捕捉语义关系，但它们可能会遗漏关键的精确匹配。幸运的是，有一种较早的技术可以帮助解决这些情况。BM25（最佳匹配 25）是一种排名函数，利用词汇匹配来寻找精确的词语或短语匹配。它对包含唯一标识符或技术术语的查询尤其有效。

BM25 的工作原理是基于 TF-IDF（术语频率-逆文档频率）概念。TF-IDF 衡量一个词对集合中文档的重要性。BM25 通过考虑文档长度并对词频应用饱和函数来改进，有助于防止常用词占据结果。

以下是 BM25 在语义嵌入无法成功的地方的成功方式：假设用户在技术支持数据库中查询“错误代码 TS-999”。嵌入模型可能会找到关于错误码的一般内容，但可能会错过准确的“TS-999”匹配。BM25 会寻找这串特定的文本字符串来识别相关文档。

通过结合嵌入和 BM25 技术，RAG 解决方案可以通过以下步骤更准确地检索最适用的块：
- 将知识库（文档的“语料库”）拆分成更小的文本块，通常不超过几百个标记;
- 为这些块创建 TF-IDF 编码和语义嵌入;
- 用 BM25 根据精确匹配找到顶层块;
- 利用嵌入根据语义相似性查找顶部块;
- 利用秩融合技术组合和去重（3）和（4）的结果;
- 将前 K 块添加到提示中以生成响应。

通过结合 BM25 和嵌入模型，传统 RAG 系统能够提供更全面、更准确的结果，在精确术语匹配与更广泛的语义理解之间取得平衡。

![img](./001.介绍上下文检索.assets/2F45603646e979c62349ce27744a940abf30200d57-3840x2160.webp)

这种方法使你能够以成本效益高的方式扩展到庞大的知识库，远超单一提示所能涵盖的范围。但这些传统 RAG 系统有一个显著的局限：它们经常破坏上下文。

# 传统 RAG 中的语境难题

在传统的 RAG 中，文档通常被拆分成更小的块，以便高效检索。虽然这种方法在许多应用中表现良好，但当单个区块缺乏足够的上下文时，可能会带来问题。

例如，假设您在知识库中嵌入了一系列财务信息（比如美国证券交易委员会的文件），并收到以下问题：“*ACME 公司在 2023 年第二季度的收入增长情况如何？”*

相关内容可能包含：“ *公司收入较上一季度增长了 3%。”* 然而，这部分单独并未说明所指的公司或相关时间段，这使得检索正确信息或有效利用信息变得困难。

# 介绍上下文检索

上下文检索解决了这个问题，通过在嵌入前为每个区块预先添加特定区块的解释上下文（“上下文嵌入”），并创建 BM25 索引（“上下文 BM25”）。

让我们回到 SEC 申报收集的例子。以下是区块可能被转换的一个示例：

```
original_chunk = "The company's revenue grew by 3% over the previous quarter."

contextualized_chunk = "This chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million. The company's revenue grew by 3% over the previous quarter."
```

值得注意的是，过去也提出了利用上下文提升检索的方法。其他提案包括：[ 在区块中添加通用文档摘要 ](https://aclanthology.org/W02-0405.pdf)（我们进行了实验，效果非常有限）、[ 假设性文档嵌入](https://arxiv.org/abs/2212.10496)以及[基于摘要的索引 ](https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec)（我们评估后表现较低）。这些方法与本文提出的不同。
# 实现上下文检索

当然，手动批注知识库中成千上万甚至数百万块的内容会非常繁琐。为了实现上下文检索，我们求助于 Claude。我们写了一个提示，要求模型提供简明、针对区块的上下文，利用整体文档的上下文来解释区块。我们使用了以下 Claude 3 俳句提示为每个段落生成上下文：

```
<document> 
{{WHOLE_DOCUMENT}} 
</document> 
Here is the chunk we want to situate within the whole document 
<chunk> 
{{CHUNK_CONTENT}} 
</chunk> 
Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else. 
```

最终生成的上下文文本，通常为 50-100 个令牌，会在嵌入块前加上，并在创建 BM25 索引之前。

以下是预处理流程在实际中的样子：

![img](./001.介绍上下文检索.assets/2F2496e7c6fedd7ffaa043895c23a4089638b0c21b-3840x2160.webp)

# 利用提示缓存降低上下文检索成本

借助我们上述提到的特殊提示缓存功能，Claude 独特地实现了低成本的上下文检索。通过提示缓存，你不需要为每个区块传递参考文档。你只需将文档加载到缓存中一次，然后引用之前缓存的内容即可。假设有 800 个令牌块、8k 令牌文档、50 个令牌上下文指令和每个区块 100 个上下文令牌， **生成上下文化区块的一次性成本为每百万个文档令牌 1.02 美元** 。

## 方法论

我们在多个知识领域进行了实验（代码库、虚构、ArXiv 论文、科学论文）、嵌入模型、检索策略和评估指标。我们在[附录 II](https://assets.anthropic.com/m/1632cded0a125333/original/Contextual-Retrieval-Appendix-2.pdf) 中包含了为每个领域使用的一些问题和答案示例。下图展示了所有知识领域在采用最佳嵌入配置（Gemini Text 004）并检索前 20 块时的平均表现。我们使用 1 减 recall@20 作为评估指标，衡量在前 20 个区块中未能检索的相关文档百分比。你可以在附录中看到完整结果——上下文化能提升我们评估的每一种嵌入-源代码组合的性能。

我们的实验显示：

- **Contextual Embeddings reduced the top-20-chunk retrieval failure rate by 35%** (5.7% → 3.7%).
  **上下文嵌入将前 20 块的检索失败率降低了 35%**（5.7%→3.7%）。
- **Combining Contextual Embeddings and Contextual BM25 reduced the top-20-chunk retrieval failure rate by 49%** (5.7% → 2.9%).
  **结合上下文嵌入和上下文 BM25，前 20 块检索失败率降低了 49%**（5.7%→2.9%）。

![img](./001.介绍上下文检索.assets/2F7f8d739e491fe6b3ba0e6a9c74e4083d760b88c9-3840x2160.webp)

## 实现考虑

在实施上下文检索时，有几个需要注意的因素：

- **区块边界：** 考虑你如何将文档拆分成多个部分。区块大小、区块边界和区块重叠的选择会影响检索性能 
- **嵌入模型：** 虽然上下文检索提升了我们测试的所有嵌入模型的性能，但某些模型受益可能更多。我们发现 [Gemini](https://ai.google.dev/gemini-api/docs/embeddings) 和 [Voyage](https://www.voyageai.com/) 嵌入尤其有效。
- **自定义上下文提示：** 虽然我们提供的通用提示效果不错，但如果你能通过针对特定领域或用例定制的提示（例如包含知识库中其他文档中可能只有定义的关键术语词汇表）来获得更好的效果。
- 区**块数量：** 在上下文窗口中添加更多块块，可以增加你包含相关信息的可能性。不过，过多的信息可能会让模型分心，所以这也是有限制的。我们尝试过交付 5 块、10 块和 20 块，发现使用 20 块是这些选项中性能最高的（详见附录对比），但根据你的使用场景进行实验还是值得的。

**一定要做评估：** 通过传递上下文区块并区分上下文和区块，可以提升响应生成。

# 通过重新排名进一步提升绩效

最后一步，我们可以将上下文检索与其他技术结合，带来更多性能提升。在传统的 RAG 中，AI 系统会搜索其知识库，以寻找潜在相关的信息块。对于庞大的知识库，这种初步检索通常会返回大量——有时是数百条——且相关性和重要性各异的数据块。

重新排序是一种常用的过滤技术，确保只有最相关的块传递给模型。重新排序能提供更好的响应，并降低成本和延迟，因为模型处理的信息更少。关键步骤包括：

- 进行初步检索以获取顶部可能相关的部分（我们使用了前150块）;
- 将前 N 个区块连同用户查询一同传递到重排序模型中;
- 使用重新排序模型，根据每个段落与提示的相关性和重要性给出评分，然后选择前 K 段（我们用了前 20 块）;
- 将顶 K 块作为上下文传递到模型中，生成最终结果。

![img](./001.介绍上下文检索.assets/2F8f82c6175a64442ceff4334b54fac2ab3436a1d1-3840x2160.webp)

## 性能改进

市场上有多种重新排名模型。我们用 [Cohere reranker](https://cohere.com/rerank) 做了测试。Voyage[ 还提供重置器 ](https://docs.voyageai.com/docs/reranker)，虽然我们没时间测试。我们的实验表明，在多个领域中，添加重新排序步骤进一步优化检索。

具体来说，我们发现重新排序的上下文嵌入和上下文 BM25 将前 20 块检索失败率降低了 67%（5.7%→1.9%）。

![img](./001.介绍上下文检索.assets/2F93a70cfbb7cca35bb8d86ea0a23bdeeb699e8e58-3840x2160.webp)

## 成本与延迟考虑

重新排序的一个重要考虑是对延迟和成本的影响，尤其是在重新排序大量区块时。因为重新排序在运行时会增加一个额外步骤，因此不可避免地会增加一定的延迟，尽管重新排序器会同时对所有区块进行评分。在重新排序更多区块以获得更好性能与减少重新排序以降低延迟和成本之间存在固有权衡。我们建议根据你的具体使用场景尝试不同的设置，找到合适的平衡点。

# 结论

我们进行了大量测试，比较上述所有技术的不同组合（嵌入模型、使用 BM25、上下文检索、重新排序工具的使用以及检索的总数#个顶 K 结果），涵盖多种不同数据集类型。以下是我们发现的摘要：

- 嵌入+BM25 比单独嵌入更好;
- Voyage 和 Gemini 在我们测试的中嵌入效果最好;
- 将前20名的分块传递给模型比仅仅传递前10名或前5名更有效;
- 为区块添加上下文大大提升了检索准确性;
- 重新排名总比不重新排序好;
- **所有这些好处叠加** ：为了最大化性能提升，我们可以将上下文嵌入（来自 Voyage 或 Gemini）与上下文 BM25 结合，加上重新排序步骤，并将 20 个块添加到提示词中。

# 附录一

以下是跨数据集、嵌入提供者、除嵌入外使用 BM25、上下文检索的使用以及检索@20 的重新排序结果分布。

详见[附录 II](https://assets.anthropic.com/m/1632cded0a125333/original/Contextual-Retrieval-Appendix-2.pdf)，了解检索@10 和@5 的细分，以及每个数据集的示例问题和答案。

![img](./001.介绍上下文检索.assets/2F646a894ec4e6120cade9951a362f685cd2ec89b2-2458x2983.webp)

> 原文链接：Contextual Retrieval in AI Systems \ Anthropic (https://www.anthropic.com/engineering/contextual-retrieval）
>
> 作者：Research and writing by Daniel Ford. Thanks to Orowa Sikder, Gautam Mittal, and Kenneth Lien for critical feedback, Samuel Flamini for implementing the cookbooks, Lauren Polansky for project coordination and Alex Albert, Susan Payne, Stuart Ritchie, and Brad Abrams for shaping this blog post.
