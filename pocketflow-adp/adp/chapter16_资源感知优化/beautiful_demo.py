#!/usr/bin/env python3
"""
èµ„æºæ„ŸçŸ¥ä¼˜åŒ–ä¸ç›‘æ§æ¼”ç¤º - ç¾åŒ–ç‰ˆ

è¯¥æ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•ç»“åˆèµ„æºæ„ŸçŸ¥ä¼˜åŒ–ç³»ç»Ÿå’Œèµ„æºç›‘æ§å™¨ï¼Œ
å®ç°æ™ºèƒ½çš„èµ„æºç®¡ç†å’Œä¼˜åŒ–ï¼Œå¹¶æä¾›ç¾è§‚çš„èµ„æºä½¿ç”¨æŠ¥å‘Šã€‚
"""

import sys
import os
import time
from typing import Dict, Any

# æ·»åŠ utilsç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'utils'))

from utils import call_llm
from pocketflow import Node, Flow

# å¯¼å…¥èµ„æºç›‘æ§å™¨
from resource_monitor import ResourceMonitor, create_resource_monitoring_flow

# å¯¼å…¥èµ„æºæ„ŸçŸ¥ä¼˜åŒ–ç³»ç»Ÿ
from resource_aware_optimization import (
    TaskClassifierNode, 
    SimpleQueryNode, 
    ReasoningQueryNode,
    ResourceAwareOptimizationFlow,
    call_llm_async
)


class MonitoredTaskClassifierNode(Node):
    """
    å¸¦ç›‘æ§çš„ä»»åŠ¡åˆ†ç±»èŠ‚ç‚¹
    """
    
    def __init__(self, monitor):
        super().__init__()
        self.monitor = monitor
        self.system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡åˆ†ç±»å™¨ï¼Œåˆ†æç”¨æˆ·æŸ¥è¯¢å¹¶è¿”å›ä»¥ä¸‹ä¸¤ä¸ªç±»åˆ«ä¹‹ä¸€ï¼š
        
        - simple: ç›´æ¥äº‹å®æ€§é—®é¢˜ï¼Œä¸éœ€è¦å¤æ‚æ¨ç†æˆ–å½“å‰äº‹ä»¶ä¿¡æ¯
        - reasoning: éœ€è¦é€»è¾‘æ¨ç†ã€æ•°å­¦è®¡ç®—æˆ–å¤šæ­¥éª¤æ¨ç†çš„é—®é¢˜
        
        è§„åˆ™:
        - å¯¹äºå¯ä»¥ç›´æ¥å›ç­”çš„äº‹å®æ€§é—®é¢˜ä½¿ç”¨'simple'
        - å¯¹äºé€»è¾‘ã€æ•°å­¦æˆ–å¤šæ­¥éª¤æ¨ç†é—®é¢˜ä½¿ç”¨'reasoning'
        
        åªè¿”å›JSONæ ¼å¼ï¼Œä¾‹å¦‚: {"classification": "simple"}
        """
    
    def prep(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        """å‡†å¤‡é˜¶æ®µï¼šè·å–ç”¨æˆ·æŸ¥è¯¢"""
        return {"query": shared.get("query", "")}
    
    def exec(self, prep_res: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œé˜¶æ®µï¼šä½¿ç”¨LLMå¯¹ä»»åŠ¡è¿›è¡Œåˆ†ç±»"""
        query = prep_res["query"]
        start_time = time.time()
        
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": query}
        ]
        
        try:
            response = call_llm(messages)
            end_time = time.time()
            response_time = end_time - start_time
            
            # è§£æå“åº”
            try:
                import json
                result = json.loads(response)
                classification = result.get("classification", "simple")
            except:
                classification = "simple"
            
            # è®°å½•èµ„æºä½¿ç”¨
            usage = self.monitor.record_usage(
                model_name="gpt-4o-mini",
                input_tokens=len(query.split()),
                output_tokens=len(response.split()),
                response_time=response_time,
                success=True,
                query_type="classification"
            )
            
            return {"classification": classification, "query": query}
        except Exception as e:
            end_time = time.time()
            response_time = end_time - start_time
            
            # è®°å½•å¤±è´¥
            self.monitor.record_usage(
                model_name="gpt-4o-mini",
                input_tokens=len(query.split()),
                output_tokens=0,
                response_time=response_time,
                success=False,
                query_type="classification"
            )
            
            # å¦‚æœåˆ†ç±»å¤±è´¥ï¼Œé»˜è®¤ä¸ºsimple
            return {"classification": "simple", "query": query, "error": str(e)}
    
    def post(self, shared: Dict[str, Any], prep_res: Dict[str, Any], exec_res: Dict[str, Any]) -> str:
        """åå¤„ç†é˜¶æ®µï¼šå°†åˆ†ç±»ç»“æœæ·»åŠ åˆ°å…±äº«çŠ¶æ€"""
        shared["classification"] = exec_res.get("classification", "simple")
        shared["query"] = exec_res.get("query", "")
        return exec_res.get("classification", "simple")


class MonitoredSimpleQueryNode(Node):
    """
    å¸¦ç›‘æ§çš„ç®€å•æŸ¥è¯¢èŠ‚ç‚¹
    """
    
    def __init__(self, monitor, model_name="gpt-4o-mini"):
        super().__init__()
        self.monitor = monitor
        self.model_name = model_name
    
    def prep(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        """å‡†å¤‡é˜¶æ®µï¼šè·å–æŸ¥è¯¢å†…å®¹"""
        return {"query": shared.get("query", "")}
    
    def exec(self, prep_res: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œé˜¶æ®µï¼šä½¿ç”¨è½»é‡çº§æ¨¡å‹å¤„ç†æŸ¥è¯¢"""
        query = prep_res["query"]
        start_time = time.time()
        
        try:
            response = call_llm(query)
            end_time = time.time()
            response_time = end_time - start_time
            
            # è®°å½•èµ„æºä½¿ç”¨
            usage = self.monitor.record_usage(
                model_name=self.model_name,
                input_tokens=len(query.split()),
                output_tokens=len(response.split()),
                response_time=response_time,
                success=True,
                query_type="simple"
            )
            
            return {
                "response": response,
                "model": self.model_name,
                "classification": "simple"
            }
        except Exception as e:
            end_time = time.time()
            response_time = end_time - start_time
            
            # è®°å½•å¤±è´¥
            self.monitor.record_usage(
                model_name=self.model_name,
                input_tokens=len(query.split()),
                output_tokens=0,
                response_time=response_time,
                success=False,
                query_type="simple"
            )
            
            return {
                "response": f"å¤„ç†ç®€å•æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}",
                "model": self.model_name,
                "classification": "simple",
                "error": str(e)
            }
    
    def post(self, shared: Dict[str, Any], prep_res: Dict[str, Any], exec_res: Dict[str, Any]) -> str:
        """åå¤„ç†é˜¶æ®µï¼šå°†ç»“æœæ·»åŠ åˆ°å…±äº«çŠ¶æ€"""
        shared["response"] = exec_res.get("response", "")
        shared["model_used"] = exec_res.get("model", self.model_name)
        shared["classification"] = exec_res.get("classification", "simple")
        return "default"


class MonitoredReasoningQueryNode(Node):
    """
    å¸¦ç›‘æ§çš„æ¨ç†æŸ¥è¯¢èŠ‚ç‚¹
    """
    
    def __init__(self, monitor, model_name="gpt-4o"):
        super().__init__()
        self.monitor = monitor
        self.model_name = model_name
    
    def prep(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        """å‡†å¤‡é˜¶æ®µï¼šè·å–æŸ¥è¯¢å†…å®¹"""
        return {"query": shared.get("query", "")}
    
    def exec(self, prep_res: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œé˜¶æ®µï¼šä½¿ç”¨å¼ºå¤§æ¨¡å‹å¤„ç†æ¨ç†ä»»åŠ¡"""
        query = prep_res["query"]
        start_time = time.time()
        
        # æ·»åŠ æ¨ç†æç¤º
        reasoning_prompt = f"""
        è¯·ä»”ç»†åˆ†æä»¥ä¸‹é—®é¢˜ï¼Œå¹¶è¿›è¡Œé€æ­¥æ¨ç†ï¼š
        
        {query}
        
        è¯·æä¾›è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆã€‚
        """
        
        try:
            response = call_llm(reasoning_prompt)
            end_time = time.time()
            response_time = end_time - start_time
            
            # è®°å½•èµ„æºä½¿ç”¨
            usage = self.monitor.record_usage(
                model_name=self.model_name,
                input_tokens=len(reasoning_prompt.split()),
                output_tokens=len(response.split()),
                response_time=response_time,
                success=True,
                query_type="reasoning"
            )
            
            return {
                "response": response,
                "model": self.model_name,
                "classification": "reasoning"
            }
        except Exception as e:
            end_time = time.time()
            response_time = end_time - start_time
            
            # è®°å½•å¤±è´¥
            self.monitor.record_usage(
                model_name=self.model_name,
                input_tokens=len(reasoning_prompt.split()),
                output_tokens=0,
                response_time=response_time,
                success=False,
                query_type="reasoning"
            )
            
            return {
                "response": f"å¤„ç†æ¨ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}",
                "model": self.model_name,
                "classification": "reasoning",
                "error": str(e)
            }
    
    def post(self, shared: Dict[str, Any], prep_res: Dict[str, Any], exec_res: Dict[str, Any]) -> str:
        """åå¤„ç†é˜¶æ®µï¼šå°†ç»“æœæ·»åŠ åˆ°å…±äº«çŠ¶æ€"""
        shared["response"] = exec_res.get("response", "")
        shared["model_used"] = exec_res.get("model", self.model_name)
        shared["classification"] = exec_res.get("classification", "reasoning")
        return "default"


class IntegratedResourceAwareFlow(Flow):
    """
    é›†æˆçš„èµ„æºæ„ŸçŸ¥ä¼˜åŒ–æµç¨‹
    ç»“åˆäº†èµ„æºæ„ŸçŸ¥ä¼˜åŒ–å’Œèµ„æºç›‘æ§åŠŸèƒ½
    """
    
    def __init__(self):
        super().__init__()
        
        # åˆ›å»ºèµ„æºç›‘æ§å™¨
        self.monitor = ResourceMonitor()
        
        # åˆ›å»ºå¸¦ç›‘æ§çš„èŠ‚ç‚¹
        self.classifier = MonitoredTaskClassifierNode(self.monitor)
        self.simple_handler = MonitoredSimpleQueryNode(self.monitor)
        self.reasoning_handler = MonitoredReasoningQueryNode(self.monitor)
        
        # è®¾ç½®æµç¨‹
        self.start(self.classifier)
        self.classifier.next(self.simple_handler, "simple")
        self.classifier.next(self.reasoning_handler, "reasoning")
    
    def prep(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        """å‡†å¤‡é˜¶æ®µï¼šåˆå§‹åŒ–å…±äº«çŠ¶æ€"""
        shared["start_time"] = time.time()
        return {}
    
    def post(self, shared: Dict[str, Any], prep_res: Dict[str, Any], exec_res: Dict[str, Any]) -> Dict[str, Any]:
        """åå¤„ç†é˜¶æ®µï¼šè®¡ç®—å¤„ç†æ—¶é—´å’Œèµ„æºä½¿ç”¨æƒ…å†µ"""
        end_time = time.time()
        processing_time = end_time - shared.get("start_time", end_time)
        
        return {
            "query": shared.get("query", ""),
            "response": shared.get("response", ""),
            "classification": shared.get("classification", ""),
            "model_used": shared.get("model_used", ""),
            "processing_time": processing_time
        }
    
    def get_monitor(self):
        """è·å–èµ„æºç›‘æ§å™¨"""
        return self.monitor


def format_report(report_data):
    """æ ¼å¼åŒ–èµ„æºä½¿ç”¨æŠ¥å‘Š"""
    metrics = report_data['metrics']
    
    report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    èµ„æºä½¿ç”¨æŠ¥å‘Š                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š æ€»ä½“ç»Ÿè®¡:
  â€¢ æ€»æˆæœ¬: ${metrics['total_cost']:.6f}
  â€¢ æ€»Tokenæ•°: {metrics['total_tokens']}
  â€¢ å¹³å‡å“åº”æ—¶é—´: {metrics['avg_response_time']:.2f}ç§’
  â€¢ æˆåŠŸç‡: {metrics['success_rate']*100:.2f}%
  â€¢ æŸ¥è¯¢æ¬¡æ•°: {metrics['query_count']}

ğŸ¤– æŒ‰æ¨¡å‹ç»Ÿè®¡:
"""
    
    for model, count in metrics['model_usage'].items():
        report += f"  â€¢ {model}: {count}æ¬¡æŸ¥è¯¢\n"
    
    report += "\nğŸ“‹ æŒ‰æŸ¥è¯¢ç±»å‹ç»Ÿè®¡:\n"
    for query_type, count in metrics['query_type_distribution'].items():
        report += f"  â€¢ {query_type}: {count}æ¬¡æŸ¥è¯¢\n"
    
    if 'optimization_suggestions' in metrics and metrics['optimization_suggestions']:
        report += "\nğŸ’¡ ä¼˜åŒ–å»ºè®®:\n"
        for suggestion in metrics['optimization_suggestions']:
            report += f"  â€¢ {suggestion}\n"
    
    return report


def demo_integrated_resource_aware():
    """æ¼”ç¤ºé›†æˆçš„èµ„æºæ„ŸçŸ¥ä¼˜åŒ–ç³»ç»Ÿ"""
    print("=== é›†æˆèµ„æºæ„ŸçŸ¥ä¼˜åŒ–ç³»ç»Ÿæ¼”ç¤º ===\n")
    
    # åˆ›å»ºæµç¨‹
    flow = IntegratedResourceAwareFlow()
    monitor = flow.get_monitor()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "æ³•å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ",  # simple
        "å¦‚æœä¸€ä¸ªæˆ¿é—´é‡Œæœ‰3åªçŒ«ï¼Œæ¯åªçŒ«æŠ“äº†2åªè€é¼ ï¼Œé‚£ä¹ˆæ€»å…±æœ‰å¤šå°‘åªè€é¼ è¢«æŠ“ï¼Ÿ",  # reasoning
        "è§£é‡Šé‡å­çº ç¼ çš„åŸç†åŠå…¶åœ¨é‡å­è®¡ç®—ä¸­çš„åº”ç”¨",  # reasoning
        "åˆ†æäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬æŠ€æœ¯æŒ‘æˆ˜å’Œä¼¦ç†è€ƒè™‘",  # reasoning
        "ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°å®ç°å¿«é€Ÿæ’åºç®—æ³•"  # simple
    ]
    
    # å¤„ç†æ¯ä¸ªæŸ¥è¯¢
    for i, query in enumerate(test_queries, 1):
        print(f"--- æµ‹è¯•æŸ¥è¯¢ {i}: {query} ---")
        
        # è¿è¡Œæµç¨‹
        shared_state = {"query": query}
        result = flow.run(shared_state)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"åˆ†ç±»: {result['classification']}")
        print(f"ä½¿ç”¨æ¨¡å‹: {result['model_used']}")
        print(f"å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
        print(f"å›ç­”: {result['response'][:200]}...")
        print()
    
    # ç”Ÿæˆèµ„æºä½¿ç”¨æŠ¥å‘Š
    print("--- èµ„æºä½¿ç”¨æŠ¥å‘Š ---")
    report_data = monitor.get_usage_report()
    formatted_report = format_report(report_data)
    print(formatted_report)


if __name__ == "__main__":
    demo_integrated_resource_aware()